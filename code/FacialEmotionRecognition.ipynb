{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/opt/opencv/lib//python3.6/site-packages/')\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec_files(path,ext):\n",
    "    content = os.listdir(path)\n",
    "    if len(content) == 0:\n",
    "        return []\n",
    "    if \".DS_Store\" in content:\n",
    "        content.remove(\".DS_Store\")\n",
    "    if content[0].endswith(ext):\n",
    "        return [path+\"/\"+file for file in content]\n",
    "    else:\n",
    "        return np.concatenate([get_rec_files(path+\"/\"+subpath,ext) for subpath in content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_rec_files(\"data/images\",\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data/images/S111/007/S111_007_00000008.png',\n",
       "       'data/images/S111/007/S111_007_00000009.png',\n",
       "       'data/images/S111/007/S111_007_00000013.png',\n",
       "       'data/images/S111/007/S111_007_00000007.png',\n",
       "       'data/images/S111/007/S111_007_00000006.png',\n",
       "       'data/images/S111/007/S111_007_00000012.png',\n",
       "       'data/images/S111/007/S111_007_00000004.png',\n",
       "       'data/images/S111/007/S111_007_00000010.png',\n",
       "       'data/images/S111/007/S111_007_00000011.png',\n",
       "       'data/images/S111/007/S111_007_00000005.png'], dtype='<U42')"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = get_rec_files(\"data/emotions\",\".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data/emotions/S111/007/S111_007_00000014_emotion.txt',\n",
       "       'data/emotions/S111/001/S111_001_00000014_emotion.txt',\n",
       "       'data/emotions/S111/006/S111_006_00000010_emotion.txt',\n",
       "       'data/emotions/S129/012/S129_012_00000011_emotion.txt',\n",
       "       'data/emotions/S129/006/S129_006_00000010_emotion.txt',\n",
       "       'data/emotions/S129/011/S129_011_00000018_emotion.txt',\n",
       "       'data/emotions/S129/002/S129_002_00000011_emotion.txt',\n",
       "       'data/emotions/S116/007/S116_007_00000017_emotion.txt',\n",
       "       'data/emotions/S116/001/S116_001_00000014_emotion.txt',\n",
       "       'data/emotions/S116/006/S116_006_00000007_emotion.txt'],\n",
       "      dtype='<U52')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_groups(emotions,images):\n",
    "    groups = []\n",
    "    for emotion in emotions:\n",
    "        emot_value = get_emot_value(emotion)\n",
    "        prefix = emotion.split(\"/\")[-1][:8]\n",
    "        f=np.frompyfunc(lambda x: prefix in x, 1,1)\n",
    "        selected_images = images[list(f(images))]\n",
    "        selected_images.sort()\n",
    "        groups += [(emot_value,selected_images)]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emot_value(path):\n",
    "    with open(path,\"r\") as f:\n",
    "        return f.read().split()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_images = merge_groups(emotions,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', array(['data/images/S111/007/S111_007_00000001.png',\n",
       "         'data/images/S111/007/S111_007_00000002.png',\n",
       "         'data/images/S111/007/S111_007_00000003.png',\n",
       "         'data/images/S111/007/S111_007_00000004.png',\n",
       "         'data/images/S111/007/S111_007_00000005.png',\n",
       "         'data/images/S111/007/S111_007_00000006.png',\n",
       "         'data/images/S111/007/S111_007_00000007.png',\n",
       "         'data/images/S111/007/S111_007_00000008.png',\n",
       "         'data/images/S111/007/S111_007_00000009.png',\n",
       "         'data/images/S111/007/S111_007_00000010.png',\n",
       "         'data/images/S111/007/S111_007_00000011.png',\n",
       "         'data/images/S111/007/S111_007_00000012.png',\n",
       "         'data/images/S111/007/S111_007_00000013.png',\n",
       "         'data/images/S111/007/S111_007_00000014.png'], dtype='<U42')),\n",
       " ('7', array(['data/images/S111/001/S111_001_00000001.png',\n",
       "         'data/images/S111/001/S111_001_00000002.png',\n",
       "         'data/images/S111/001/S111_001_00000003.png',\n",
       "         'data/images/S111/001/S111_001_00000004.png',\n",
       "         'data/images/S111/001/S111_001_00000005.png',\n",
       "         'data/images/S111/001/S111_001_00000006.png',\n",
       "         'data/images/S111/001/S111_001_00000007.png',\n",
       "         'data/images/S111/001/S111_001_00000008.png',\n",
       "         'data/images/S111/001/S111_001_00000009.png',\n",
       "         'data/images/S111/001/S111_001_00000010.png',\n",
       "         'data/images/S111/001/S111_001_00000011.png',\n",
       "         'data/images/S111/001/S111_001_00000012.png',\n",
       "         'data/images/S111/001/S111_001_00000013.png',\n",
       "         'data/images/S111/001/S111_001_00000014.png'], dtype='<U42'))]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_images[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_count = [emot for emot,image in group_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1': 45, '2': 18, '3': 59, '4': 25, '5': 69, '6': 28, '7': 83})"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(emot_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FER2013 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/fer2013.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"intensity\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Usage</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PrivateTest</th>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PublicTest</th>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>28709</td>\n",
       "      <td>28709</td>\n",
       "      <td>28709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             emotion  pixels  intensity\n",
       "Usage                                  \n",
       "PrivateTest     3589    3589       3589\n",
       "PublicTest      3589    3589       3589\n",
       "Training       28709   28709      28709"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Usage\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(images):\n",
    "    faceCascade = cv2.CascadeClassifier(\"data/face_recog.xml\")\n",
    "    prepared_images = []\n",
    "    \n",
    "    for path in images:\n",
    "        \n",
    "        image = cv2.imread(path)\n",
    "        \n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            image,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(48,48),\n",
    "            flags = cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        for (x, y, w, h) in faces:\n",
    "            crop_img = image[y:y+h, x:x+w]\n",
    "            \n",
    "        res_img = cv2.resize(cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY), (48,48)) \n",
    "        \n",
    "        prepared_images += [(res_img,path)]\n",
    "        \n",
    "    return prepared_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_faces = []\n",
    "for emot,pictures in group_images:\n",
    "    group_faces += [(emot,prepare_images(pictures))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_data = []\n",
    "for emot,pictures in group_faces:\n",
    "    for picture,path in pictures:\n",
    "        ck_data += [[emot,np.concatenate(picture),int(path.split(\"_\")[-1][:-4])]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_df = pd.DataFrame(data=ck_data,columns=[\"emotion\",\"pixels\",\"intensity\"])\n",
    "ck_df[\"Usage\"] = \"CK+\"\n",
    "ck_df[\"emotion\"] = pd.to_numeric(ck_df[\"emotion\"])\n",
    "ck_df[\"pixels\"] = [\" \".join([str(string) for string in pixels]) for pixels in ck_df[\"pixels\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>intensity</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>44 34 21 0 0 0 0 0 0 2 7 15 18 36 34 35 47 60 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>42 36 3 0 0 0 0 0 0 0 5 15 22 31 32 37 52 60 8...</td>\n",
       "      <td>2</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>42 35 26 0 0 0 0 0 0 0 4 8 16 32 34 33 42 58 8...</td>\n",
       "      <td>3</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>43 34 10 1 0 0 0 0 0 2 7 13 19 36 32 36 44 62 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>43 40 38 0 0 0 0 0 0 1 6 9 15 28 31 34 42 63 7...</td>\n",
       "      <td>5</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels  intensity Usage\n",
       "0        3  44 34 21 0 0 0 0 0 0 2 7 15 18 36 34 35 47 60 ...          1   CK+\n",
       "1        3  42 36 3 0 0 0 0 0 0 0 5 15 22 31 32 37 52 60 8...          2   CK+\n",
       "2        3  42 35 26 0 0 0 0 0 0 0 4 8 16 32 34 33 42 58 8...          3   CK+\n",
       "3        3  43 34 10 1 0 0 0 0 0 2 7 13 19 36 32 36 44 62 ...          4   CK+\n",
       "4        3  43 40 38 0 0 0 0 0 0 1 6 9 15 28 31 34 42 63 7...          5   CK+"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Save to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for emot,pictures in group_faces:\n",
    "    for pixel,path in pictures:\n",
    "        file_name = path.split(\"/\")[-1]\n",
    "        name = file_name.split(\".\")[0]\n",
    "        \n",
    "        face_path = \"data/faces/\"+name+\"_\"+emot\n",
    "        np.save(face_path,pixel)\n",
    "        paths += [(emot,face_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.concat([df,ck_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FER2013\n",
    "Emotion labels in the dataset:\n",
    "- 0: -4593 images- Angry\n",
    "- 1: -547 images- Disgust\n",
    "- 2: -5121 images- Fear\n",
    "- 3: -8989 images- Happy\n",
    "- 4: -6077 images- Sad\n",
    "- 5: -4002 images- Surprise\n",
    "- 6: -6198 images- Neutral\n",
    "\n",
    "CK+\n",
    "- {0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go with the FER2013 emotion standard\n",
    "emotion_map = {\n",
    "    0:6,\n",
    "    1:0,\n",
    "    3:1,\n",
    "    4:2,\n",
    "    5:3,\n",
    "    6:4,\n",
    "    7:5\n",
    "}\n",
    "\n",
    "delete_label = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_emotions(row):\n",
    "    if row[\"Usage\"] == \"CK+\":\n",
    "        row[\"emotion\"] = emotion_map[row[\"emotion\"]]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = complete_df.drop(complete_df.loc[(complete_df[\"emotion\"] == delete_label) & (complete_df[\"intensity\"] != -1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = complete_df.apply(lambda x: map_emotions(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.to_csv(\"data/face_dataset.csv\",sep=\"|\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_float_list(row):\n",
    "    row[\"pixels\"] = np.array([float(string) for string in row[\"pixels\"].split()])\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data/face_dataset.csv\",sep=\"|\")\n",
    "dataset = dataset.apply(lambda x: string_to_float_list(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x187c742470>"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187c8c8d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape([int(x) for x in dataset[\"pixels\"][5]],(48,48)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "\n",
    "    #tf_split = tf.string_split(features,delimiter=' ').values\n",
    "    #features_split = tf.cast(tf.reshape(tf_split,[48*48]),dtype=tf.float32)\n",
    "    #features_split = tf.cast(features,dtype=tf.float32)\n",
    "\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](emotion_cnn1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_layer(inputs,filters,kernels,padding,strides,pools,name,mode):\n",
    "    #print(inputs,filters,kernels,padding,strides,pools,name,mode)\n",
    "    conv_layer = tf.contrib.layers.conv2d(\n",
    "        inputs=inputs,\n",
    "        num_outputs=filters,\n",
    "        kernel_size=kernels,\n",
    "        padding=padding,\n",
    "        stride=1,\n",
    "        activation_fn=tf.nn.relu,\n",
    "        normalizer_fn=tf.contrib.layers.batch_norm\n",
    "    )\n",
    "    \n",
    "    #batch_norm = tf.contrib.layers.batch_norm(\n",
    "    #    inputs=conv_layer)\n",
    "    \n",
    "    relu_act = tf.nn.relu(\n",
    "        features=conv_layer)\n",
    "    \n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=relu_act,\n",
    "        #rate=0.4,\n",
    "        training = mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    pooling = tf.layers.max_pooling2d(\n",
    "        inputs=dropout,\n",
    "        pool_size=pools,\n",
    "        strides=strides)\n",
    "    \n",
    "    return pooling\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fc_layer(inputs,neurons,mode):\n",
    "    fc_layer = tf.contrib.layers.fully_connected(\n",
    "        inputs=inputs,\n",
    "        num_outputs=neurons\n",
    "    )\n",
    "    \n",
    "    batch_norm = tf.contrib.layers.batch_norm(\n",
    "        inputs=fc_layer)\n",
    "    \n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=batch_norm,\n",
    "        #rate=0.4,\n",
    "        training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    relu_act = tf.nn.relu(\n",
    "        features=dropout)\n",
    "\n",
    "    return relu_act\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    n_conv = [(2,3,64),(2,5,128),(2,3,512)]\n",
    "    n_fc = [256,512]\n",
    "    \n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(tf.cast(features,tf.float32), [-1, 48, 48, 1])\n",
    "    one_hot_labels = tf.one_hot(labels,7)\n",
    "    \n",
    "    conv_layer = input_layer\n",
    "    \n",
    "    for s,k,f in n_conv:\n",
    "        conv_layer = create_conv_layer(\n",
    "            inputs=conv_layer,\n",
    "            filters=f,\n",
    "            kernels=[k,k],\n",
    "            padding=\"same\",\n",
    "            strides=s,\n",
    "            pools=[2,2],\n",
    "            name=\"conv\"+str(f),\n",
    "            mode=mode\n",
    "            )\n",
    "        \n",
    "    \n",
    "    \n",
    "    fc_layer = tf.reshape(conv_layer,[-1,6*6*512])\n",
    "    \n",
    "    for n in n_fc:\n",
    "        fc_layer = create_fc_layer(\n",
    "            inputs=fc_layer,\n",
    "            neurons=n,\n",
    "            mode=mode\n",
    "        )\n",
    "        print(fc_layer)\n",
    "        \n",
    "    logits = tf.layers.dense(inputs=fc_layer, units=7)\n",
    "    \n",
    "    softmax = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": softmax\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    print(labels,logits)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'tmp/mnist_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x187993df60>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "emotion_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"tmp/mnist_convnet_model\")\n",
    "\n",
    "\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=2000)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tensor(series):\n",
    "    return np.concatenate(series.values).reshape(len(series),len(series[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_to_tensor(dataset[\"pixels\"])\n",
    "y = dataset[\"emotion\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "shuffle() missing 1 required positional argument: 'buffer_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-509-9f22fd7d74a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         64),\n\u001b[1;32m      6\u001b[0m     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     hooks=[logging_hook])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    841\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    851\u001b[0m       features, labels, input_hooks = (\n\u001b[1;32m    852\u001b[0m           self._get_features_and_labels_from_input_fn(\n\u001b[0;32m--> 853\u001b[0;31m               input_fn, model_fn_lib.ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m    854\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m       estimator_spec = self._call_model_fn(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m    689\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_features_and_labels_from_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m     \u001b[0;31m# TODO(anjalisridhar): What about the default DistributionStrategy? Perhaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;31m# using any input is alright in that case. There is also a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-509-9f22fd7d74a8>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         64),\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m130\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     hooks=[logging_hook])\n",
      "\u001b[0;32m<ipython-input-508-f7e1482bd31b>\u001b[0m in \u001b[0;36mtrain_input_fn\u001b[0;34m(features, labels, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Shuffle, repeat, and batch the examples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Return the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: shuffle() missing 1 required positional argument: 'buffer_size'"
     ]
    }
   ],
   "source": [
    "emotion_classifier.train(\n",
    "    input_fn=lambda:train_input_fn(\n",
    "        x,\n",
    "        y,\n",
    "        64),\n",
    "    steps=130,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41297, 2304)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.concatenate(dataset[\"pixels\"].values).reshape(len(dataset[\"pixels\"]),len(dataset[\"pixels\"][0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
