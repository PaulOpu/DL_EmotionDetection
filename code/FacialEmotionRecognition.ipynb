{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/usr/local/opt/opencv/lib//python3.6/site-packages/')\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec_files(path,ext):\n",
    "    content = os.listdir(path)\n",
    "    if len(content) == 0:\n",
    "        return []\n",
    "    if \".DS_Store\" in content:\n",
    "        content.remove(\".DS_Store\")\n",
    "    if content[0].endswith(ext):\n",
    "        return [path+\"/\"+file for file in content]\n",
    "    else:\n",
    "        return np.concatenate([get_rec_files(path+\"/\"+subpath,ext) for subpath in content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_rec_files(\"data/images\",\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data/images/S111/007/S111_007_00000008.png',\n",
       "       'data/images/S111/007/S111_007_00000009.png',\n",
       "       'data/images/S111/007/S111_007_00000013.png',\n",
       "       'data/images/S111/007/S111_007_00000007.png',\n",
       "       'data/images/S111/007/S111_007_00000006.png',\n",
       "       'data/images/S111/007/S111_007_00000012.png',\n",
       "       'data/images/S111/007/S111_007_00000004.png',\n",
       "       'data/images/S111/007/S111_007_00000010.png',\n",
       "       'data/images/S111/007/S111_007_00000011.png',\n",
       "       'data/images/S111/007/S111_007_00000005.png'], dtype='<U42')"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = get_rec_files(\"data/emotions\",\".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data/emotions/S111/007/S111_007_00000014_emotion.txt',\n",
       "       'data/emotions/S111/001/S111_001_00000014_emotion.txt',\n",
       "       'data/emotions/S111/006/S111_006_00000010_emotion.txt',\n",
       "       'data/emotions/S129/012/S129_012_00000011_emotion.txt',\n",
       "       'data/emotions/S129/006/S129_006_00000010_emotion.txt',\n",
       "       'data/emotions/S129/011/S129_011_00000018_emotion.txt',\n",
       "       'data/emotions/S129/002/S129_002_00000011_emotion.txt',\n",
       "       'data/emotions/S116/007/S116_007_00000017_emotion.txt',\n",
       "       'data/emotions/S116/001/S116_001_00000014_emotion.txt',\n",
       "       'data/emotions/S116/006/S116_006_00000007_emotion.txt'],\n",
       "      dtype='<U52')"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_groups(emotions,images):\n",
    "    groups = []\n",
    "    for emotion in emotions:\n",
    "        emot_value = get_emot_value(emotion)\n",
    "        prefix = emotion.split(\"/\")[-1][:8]\n",
    "        f=np.frompyfunc(lambda x: prefix in x, 1,1)\n",
    "        selected_images = images[list(f(images))]\n",
    "        selected_images.sort()\n",
    "        groups += [(emot_value,selected_images)]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emot_value(path):\n",
    "    with open(path,\"r\") as f:\n",
    "        return f.read().split()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_images = merge_groups(emotions,images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', array(['data/images/S111/007/S111_007_00000001.png',\n",
       "         'data/images/S111/007/S111_007_00000002.png',\n",
       "         'data/images/S111/007/S111_007_00000003.png',\n",
       "         'data/images/S111/007/S111_007_00000004.png',\n",
       "         'data/images/S111/007/S111_007_00000005.png',\n",
       "         'data/images/S111/007/S111_007_00000006.png',\n",
       "         'data/images/S111/007/S111_007_00000007.png',\n",
       "         'data/images/S111/007/S111_007_00000008.png',\n",
       "         'data/images/S111/007/S111_007_00000009.png',\n",
       "         'data/images/S111/007/S111_007_00000010.png',\n",
       "         'data/images/S111/007/S111_007_00000011.png',\n",
       "         'data/images/S111/007/S111_007_00000012.png',\n",
       "         'data/images/S111/007/S111_007_00000013.png',\n",
       "         'data/images/S111/007/S111_007_00000014.png'], dtype='<U42')),\n",
       " ('7', array(['data/images/S111/001/S111_001_00000001.png',\n",
       "         'data/images/S111/001/S111_001_00000002.png',\n",
       "         'data/images/S111/001/S111_001_00000003.png',\n",
       "         'data/images/S111/001/S111_001_00000004.png',\n",
       "         'data/images/S111/001/S111_001_00000005.png',\n",
       "         'data/images/S111/001/S111_001_00000006.png',\n",
       "         'data/images/S111/001/S111_001_00000007.png',\n",
       "         'data/images/S111/001/S111_001_00000008.png',\n",
       "         'data/images/S111/001/S111_001_00000009.png',\n",
       "         'data/images/S111/001/S111_001_00000010.png',\n",
       "         'data/images/S111/001/S111_001_00000011.png',\n",
       "         'data/images/S111/001/S111_001_00000012.png',\n",
       "         'data/images/S111/001/S111_001_00000013.png',\n",
       "         'data/images/S111/001/S111_001_00000014.png'], dtype='<U42'))]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_images[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_count = [emot for emot,image in group_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1': 45, '2': 18, '3': 59, '4': 25, '5': 69, '6': 28, '7': 83})"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(emot_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FER2013 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/fer2013.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"intensity\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Usage</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PrivateTest</th>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PublicTest</th>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "      <td>3589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Training</th>\n",
       "      <td>28709</td>\n",
       "      <td>28709</td>\n",
       "      <td>28709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             emotion  pixels  intensity\n",
       "Usage                                  \n",
       "PrivateTest     3589    3589       3589\n",
       "PublicTest      3589    3589       3589\n",
       "Training       28709   28709      28709"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Usage\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_images(images):\n",
    "    faceCascade = cv2.CascadeClassifier(\"data/face_recog.xml\")\n",
    "    prepared_images = []\n",
    "    \n",
    "    for path in images:\n",
    "        \n",
    "        image = cv2.imread(path)\n",
    "        \n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            image,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(48,48),\n",
    "            flags = cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "        for (x, y, w, h) in faces:\n",
    "            crop_img = image[y:y+h, x:x+w]\n",
    "            \n",
    "        res_img = cv2.resize(cv2.cvtColor(crop_img, cv2.COLOR_BGR2GRAY), (48,48)) \n",
    "        \n",
    "        prepared_images += [(res_img,path)]\n",
    "        \n",
    "    return prepared_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_faces = []\n",
    "for emot,pictures in group_images:\n",
    "    group_faces += [(emot,prepare_images(pictures))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_data = []\n",
    "for emot,pictures in group_faces:\n",
    "    for picture,path in pictures:\n",
    "        ck_data += [[emot,np.concatenate(picture),int(path.split(\"_\")[-1][:-4])]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_df = pd.DataFrame(data=ck_data,columns=[\"emotion\",\"pixels\",\"intensity\"])\n",
    "ck_df[\"Usage\"] = \"CK+\"\n",
    "ck_df[\"emotion\"] = pd.to_numeric(ck_df[\"emotion\"])\n",
    "ck_df[\"pixels\"] = [\" \".join([str(string) for string in pixels]) for pixels in ck_df[\"pixels\"].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>intensity</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>44 34 21 0 0 0 0 0 0 2 7 15 18 36 34 35 47 60 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>42 36 3 0 0 0 0 0 0 0 5 15 22 31 32 37 52 60 8...</td>\n",
       "      <td>2</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>42 35 26 0 0 0 0 0 0 0 4 8 16 32 34 33 42 58 8...</td>\n",
       "      <td>3</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>43 34 10 1 0 0 0 0 0 2 7 13 19 36 32 36 44 62 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>43 40 38 0 0 0 0 0 0 1 6 9 15 28 31 34 42 63 7...</td>\n",
       "      <td>5</td>\n",
       "      <td>CK+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels  intensity Usage\n",
       "0        3  44 34 21 0 0 0 0 0 0 2 7 15 18 36 34 35 47 60 ...          1   CK+\n",
       "1        3  42 36 3 0 0 0 0 0 0 0 5 15 22 31 32 37 52 60 8...          2   CK+\n",
       "2        3  42 35 26 0 0 0 0 0 0 0 4 8 16 32 34 33 42 58 8...          3   CK+\n",
       "3        3  43 34 10 1 0 0 0 0 0 2 7 13 19 36 32 36 44 62 ...          4   CK+\n",
       "4        3  43 40 38 0 0 0 0 0 0 1 6 9 15 28 31 34 42 63 7...          5   CK+"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ck_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Save to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = []\n",
    "for emot,pictures in group_faces:\n",
    "    for pixel,path in pictures:\n",
    "        file_name = path.split(\"/\")[-1]\n",
    "        name = file_name.split(\".\")[0]\n",
    "        \n",
    "        face_path = \"data/faces/\"+name+\"_\"+emot\n",
    "        np.save(face_path,pixel)\n",
    "        paths += [(emot,face_path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.concat([df,ck_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FER2013\n",
    "Emotion labels in the dataset:\n",
    "- 0: -4593 images- Angry\n",
    "- 1: -547 images- Disgust\n",
    "- 2: -5121 images- Fear\n",
    "- 3: -8989 images- Happy\n",
    "- 4: -6077 images- Sad\n",
    "- 5: -4002 images- Surprise\n",
    "- 6: -6198 images- Neutral\n",
    "\n",
    "CK+\n",
    "- {0=neutral, 1=anger, 2=contempt, 3=disgust, 4=fear, 5=happy, 6=sadness, 7=surprise}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go with the FER2013 emotion standard\n",
    "emotion_map = {\n",
    "    0:6,\n",
    "    1:0,\n",
    "    3:1,\n",
    "    4:2,\n",
    "    5:3,\n",
    "    6:4,\n",
    "    7:5\n",
    "}\n",
    "\n",
    "delete_label = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_emotions(row):\n",
    "    if row[\"Usage\"] == \"CK+\":\n",
    "        row[\"emotion\"] = emotion_map[row[\"emotion\"]]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = complete_df.drop(complete_df.loc[(complete_df[\"emotion\"] == delete_label) & (complete_df[\"intensity\"] != -1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = complete_df.apply(lambda x: map_emotions(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.to_csv(\"data/face_dataset.csv\",sep=\"|\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_float_list(row):\n",
    "    row[\"pixels\"] = np.array([float(string) for string in row[\"pixels\"].split()])\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"~/data/emotion_reco/face_dataset.csv\",sep=\"|\")\n",
    "dataset = dataset.apply(lambda x: string_to_float_list(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f87fd14ea20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.reshape([int(x) for x in dataset[\"pixels\"][5]],(48,48)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "\n",
    "    #tf_split = tf.string_split(features,delimiter=' ').values\n",
    "    #features_split = tf.cast(tf.reshape(tf_split,[48*48]),dtype=tf.float32)\n",
    "    #features_split = tf.cast(features,dtype=tf.float32)\n",
    "\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels=None, batch_size=None):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # Convert inputs to a tf.dataset object.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the read end of the pipeline.\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](emotion_cnn1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_layer(inputs,filters,kernels,padding,strides,pools,name,mode):\n",
    "    #print(inputs,filters,kernels,padding,strides,pools,name,mode)\n",
    "    conv_layer = tf.contrib.layers.conv2d(\n",
    "        inputs=inputs,\n",
    "        num_outputs=filters,\n",
    "        kernel_size=kernels,\n",
    "        padding=padding,\n",
    "        stride=1,\n",
    "        activation_fn=tf.nn.relu,\n",
    "        normalizer_fn=tf.contrib.layers.batch_norm\n",
    "    )\n",
    "    \n",
    "    #batch_norm = tf.contrib.layers.batch_norm(\n",
    "    #    inputs=conv_layer)\n",
    "    \n",
    "    relu_act = tf.nn.relu(\n",
    "        features=conv_layer)\n",
    "    \n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=relu_act,\n",
    "        #rate=0.4,\n",
    "        training = mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    pooling = tf.layers.max_pooling2d(\n",
    "        inputs=dropout,\n",
    "        pool_size=pools,\n",
    "        strides=strides)\n",
    "    \n",
    "    return pooling\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fc_layer(inputs,neurons,mode):\n",
    "    fc_layer = tf.contrib.layers.fully_connected(\n",
    "        inputs=inputs,\n",
    "        num_outputs=neurons\n",
    "    )\n",
    "    \n",
    "    batch_norm = tf.contrib.layers.batch_norm(\n",
    "        inputs=fc_layer)\n",
    "    \n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=batch_norm,\n",
    "        #rate=0.4,\n",
    "        training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    relu_act = tf.nn.relu(\n",
    "        features=dropout)\n",
    "\n",
    "    return relu_act\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    n_conv = [(2,3,64),(2,5,128),(2,3,512)]\n",
    "    n_fc = [256,512]\n",
    "    \n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(tf.cast(features,tf.float32), [-1, 48, 48, 1])\n",
    "    one_hot_labels = tf.one_hot(labels,7)\n",
    "    \n",
    "    conv_layer = input_layer\n",
    "    \n",
    "    for s,k,f in n_conv:\n",
    "        conv_layer = create_conv_layer(\n",
    "            inputs=conv_layer,\n",
    "            filters=f,\n",
    "            kernels=[k,k],\n",
    "            padding=\"same\",\n",
    "            strides=s,\n",
    "            pools=[2,2],\n",
    "            name=\"conv\"+str(f),\n",
    "            mode=mode\n",
    "            )\n",
    "        \n",
    "    \n",
    "    \n",
    "    fc_layer = tf.reshape(conv_layer,[-1,6*6*512])\n",
    "    \n",
    "    for n in n_fc:\n",
    "        fc_layer = create_fc_layer(\n",
    "            inputs=fc_layer,\n",
    "            neurons=n,\n",
    "            mode=mode\n",
    "        )\n",
    "        print(fc_layer)\n",
    "        \n",
    "    logits = tf.layers.dense(inputs=fc_layer, units=7)\n",
    "    \n",
    "    softmax = tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": softmax\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    print(labels,logits)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_session_config': None, '_task_type': 'worker', '_num_worker_replicas': 1, '_save_checkpoints_secs': 600, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f87ff859b70>, '_keep_checkpoint_max': 5, '_evaluation_master': '', '_model_dir': 'tmp/mnist_convnet_model', '_global_id_in_cluster': 0, '_service': None, '_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_task_id': 0, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_train_distribute': None}\n"
     ]
    }
   ],
   "source": [
    "emotion_classifier = tf.estimator.Estimator(\n",
    "    model_fn=cnn_model_fn, model_dir=\"tmp/mnist_convnet_model\")\n",
    "\n",
    "\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=2000)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_tensor(series):\n",
    "    return np.concatenate(series.values).reshape(len(series),len(series[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_to_tensor(dataset[\"pixels\"])\n",
    "y = dataset[\"emotion\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"Relu_3:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"Relu_4:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int64) Tensor(\"dense/BiasAdd:0\", shape=(?, 7), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/mnist_convnet_model/model.ckpt-2690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "emotion_classifier.train(\n",
    "    input_fn=lambda:train_input_fn(\n",
    "        x,\n",
    "        y,\n",
    "        128),\n",
    "    steps=128*10,\n",
    "    hooks=[logging_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41297, 2304)"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.concatenate(dataset[\"pixels\"].values).reshape(len(dataset[\"pixels\"]),len(dataset[\"pixels\"][0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"Relu_3:0\", shape=(?, 256), dtype=float32)\n",
      "Tensor(\"Relu_4:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"IteratorGetNext:1\", shape=(?,), dtype=int64, device=/device:CPU:0) Tensor(\"dense/BiasAdd:0\", shape=(?, 7), dtype=float32)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-06-18-18:34:53\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from tmp/mnist_convnet_model/model.ckpt-2690\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-06-18-18:34:58\n",
      "INFO:tensorflow:Saving dict for global step 2690: accuracy = 0.4743686, global_step = 2690, loss = 1.7401546\n"
     ]
    }
   ],
   "source": [
    "eval_results = emotion_classifier.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(x,y,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
